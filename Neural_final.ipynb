{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqi8Kbf6f0RI"
      },
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "class DeepNN:\n",
        "    first_layer = {} ##dictionary for weight and bias of 1st layer\n",
        "    second_layer = {} ##dictionary for weight and bias of 1st layer\n",
        "\n",
        "    def __init__(self, inputs, hidden, outputs):\n",
        "        ## initialize the model weights and biases of the first and second hidden layer \n",
        "        ##hidden:no. of neurones in each layer(taken equal)\n",
        "        self.first_layer['weight'] = np.random.randn(hidden,inputs) / np.sqrt(inputs)\n",
        "        self.first_layer['bias'] = np.random.randn(hidden,1) / np.sqrt(hidden)\n",
        "        self.second_layer['weight'] = np.random.randn(outputs,hidden) / np.sqrt(hidden)\n",
        "        self.second_layer['bias'] = np.random.randn(outputs,1) / np.sqrt(hidden)\n",
        "        self.input_size = inputs\n",
        "        self.hid_size = hidden\n",
        "        self.output_size = outputs\n",
        "\n",
        "    def __activfunc(self,Z,type = 'ReLU',deri = False):##2 possible activation functions can be taken\n",
        "        # implement the activation function\n",
        "        if type == 'ReLU':\n",
        "            if deri == True:\n",
        "                return np.array([1 if i>0 else 0 for i in np.squeeze(Z)])\n",
        "            else:\n",
        "                return np.array([i if i>0 else 0 for i in np.squeeze(Z)])\n",
        "        elif type == 'Sigmoid':\n",
        "            if deri == True:\n",
        "                return 1/(1+np.exp(-Z))*(1-1/(1+np.exp(-Z)))\n",
        "            else:\n",
        "                return 1/(1+np.exp(-Z))\n",
        "        \n",
        "    def __Softmax(self,z): ## softmax function\n",
        "        return 1/sum(np.exp(z)) * np.exp(z)\n",
        "\n",
        "    def __cross_entropy_error(self,v,y): ## cross entropy error\n",
        "        return -np.log(v[y])\n",
        "\n",
        "    def __forward(self,x,y): ##forward process,find prediction list and respective costs\n",
        "    \n",
        "        Z = np.matmul(self.first_layer['weight'],x).reshape((self.hid_size,1)) + self.first_layer['bias']##unactivated output of first layer\n",
        "        H = np.array(self.__activfunc(Z)).reshape((self.hid_size,1))##1st layer output activated\n",
        "        U = np.matmul(self.second_layer['weight'],H).reshape((self.output_size,1)) + self.second_layer['bias']##unactivated output of second layer\n",
        "        predict_list = np.squeeze(self.__Softmax(U))##activated output of second layer and = the final output\n",
        "        error = self.__cross_entropy_error(predict_list,y)\n",
        "        \n",
        "        dic = {\n",
        "            'Z':Z,\n",
        "            'H':H,\n",
        "            'U':U,\n",
        "            'f_X':predict_list.reshape((1,self.output_size)),\n",
        "            'error':error\n",
        "        }\n",
        "        return dic\n",
        "\n",
        "    def __back_propagation(self,x,y,f_result):## back propagation process to compute the gradients\n",
        "        E = np.array([0]*self.output_size).reshape((1,self.output_size))##vector with all 0 elements..here of size 10X1\n",
        "        E[0][y] = 1 ##one_hot\n",
        "        dU = (-(E - f_result['f_X'])).reshape((self.output_size,1))## matrix subtraction for each of the labels\n",
        "        db2 = copy.copy(dU) ##d(b2)=dU\n",
        "        dW2 = np.matmul(dU,f_result['H'].transpose())##dW=matrix mult of gradient of output and trnspose of input...elementwise\n",
        "        delta = np.matmul(self.second_layer['weight'].transpose(),dU)\n",
        "        db1 = delta.reshape(self.hid_size,1)*self.__activfunc(f_result['Z'],deri=True).reshape(self.hid_size,1)##error wrt input=error wrt output @ derivative of activation function\n",
        "        dW1 = np.matmul(db1.reshape((self.hid_size,1)),x.reshape((1,784)))##dW=matrix mult of gradient of output and trnspose of input...elementwise\n",
        "\n",
        "        grad = {\n",
        "            'dW2':dW2,\n",
        "            'db2':db2,\n",
        "            'db1':db1,\n",
        "            'dW1':dW1\n",
        "        }\n",
        "        return grad ## dictionary of gradients\n",
        "\n",
        "    def __optimize(self,b_result, learning_rate): ## update the parameters\n",
        "        self.second_layer['weight'] -= learning_rate*b_result['dW2']\n",
        "        self.second_layer['bias'] -= learning_rate*b_result['db2']\n",
        "        self.first_layer['bias'] -= learning_rate*b_result['db1']\n",
        "        self.first_layer['weight'] -= learning_rate*b_result['dW1']\n",
        "\n",
        "    def __loss(self,x_train,y_train):## implement the loss function on the entire training set\n",
        "        loss = 0\n",
        "        for i in range(len(x_train)):\n",
        "            y = y_train[i]\n",
        "            x = x_train[i][:]\n",
        "            loss += self.__forward(x,y)['error']\n",
        "        return loss\n",
        "\n",
        "    def train(self, x_train, y_train, num_iterations = 1000, learning_rate = 0.5): ## generate a random list of indices for the training set\n",
        "        rand_indices = np.random.choice(len(x_train), num_iterations, replace=True)\n",
        "        \n",
        "        def l_rate(base_rate, ite, num_iterations, schedule = False):##learning scheduler\n",
        "            if schedule == True:\n",
        "                return base_rate * 10 ** (-np.floor(ite/num_iterations*5))\n",
        "            else:\n",
        "                return base_rate\n",
        "\n",
        "        count = 1\n",
        "        loss_dict = {}\n",
        "        test_dict = {}\n",
        "\n",
        "        for i in rand_indices:\n",
        "            f_result = self.__forward(x_train[i],y_train[i])\n",
        "            b_result = self.__back_propagation(x_train[i],y_train[i],f_result)\n",
        "            self.__optimize(b_result,l_rate(learning_rate,i,num_iterations,True))\n",
        "            \n",
        "            if count % 1000 == 0:\n",
        "                if count % 5000 == 0: ##shows loss and accuracy every 5000 iterations\n",
        "                    loss = self.__loss(x_train,y_train)\n",
        "                    test = self.testing(x_test,y_test)\n",
        "                    print('Training for {} iterations complete,'.format(count),'Loss = {}'.format(loss))\n",
        "                    loss_dict[str(count)]=loss\n",
        "                    test_dict[str(count)]=test\n",
        "                else: ##shows progress every 1000 iters\n",
        "                    print('Training for {} iterations complete,'.format(count))\n",
        "            count += 1\n",
        "\n",
        "        print('Training finished!')\n",
        "        return loss_dict, test_dict\n",
        "\n",
        "    def testing(self,x_test, y_test): ## test the model on the training dataset\n",
        "        total_correct = 0\n",
        "        for n in range(len(x_test)):\n",
        "            y = y_test[n]\n",
        "            x = x_test[n][:]\n",
        "            prediction = np.argmax(self.__forward(x,y)['f_X'])\n",
        "            if (prediction == y):\n",
        "                total_correct += 1\n",
        "        print('Accuarcy Test: ',total_correct*100/len(x_test))\n",
        "        return total_correct/np.float(len(x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8yMEllOeXMa"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train=pd.read_csv('sample_data/mnist_train_small.csv')\n",
        "test=pd.read_csv('sample_data/mnist_test.csv')\n",
        "\n",
        "df = pd.DataFrame(train)\n",
        "\n",
        "dff = pd.DataFrame(test)\n",
        "\n",
        "##extracting y out of dataset\n",
        "y_train=df.iloc[:,0]\n",
        "y_test=dff.iloc[:,0]\n",
        "\n",
        "##extracting x out of dataset\n",
        "x_train=df.drop(df.columns[0], axis=1)\n",
        "x_test=dff.drop(dff.columns[0], axis=1)\n",
        "\n",
        "\n",
        "##converting dataframe to numpy array\n",
        "x_train.to_numpy()\n",
        "x_test.to_numpy()\n",
        "y_train.to_numpy().astype('int')\n",
        "y_test.to_numpy().astype('int')\n",
        "\n",
        "##normalise x\n",
        "x_train=x_train/255.0\n",
        "x_test=x_test/255.0\n",
        "\n",
        "##y_train=pd.get_dummies(y_train).values\n",
        "x_train=x_train.values\n",
        "x_test=x_test.values\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbGnUmoAgbSU",
        "outputId": "1ea8d7a9-87c1-4dd0-8783-44d2831829b4"
      },
      "source": [
        "num_iterations = 200000\n",
        "learning_rate = 0.01\n",
        "num_inputs = 28*28\n",
        "num_outputs = 10\n",
        "hidden_size = 300\n",
        "\n",
        "# data fitting, training and accuracy evaluation\n",
        "model = DeepNN(num_inputs,hidden_size,num_outputs)\n",
        "cost_dict, tests_dict = model.train(x_train,y_train,num_iterations=num_iterations,learning_rate=learning_rate)\n",
        "accu = model.testing(x_test,y_test)   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for 1000 iterations complete,\n",
            "Training for 2000 iterations complete,\n",
            "Training for 3000 iterations complete,\n",
            "Training for 4000 iterations complete,\n",
            "Accuarcy Test:  89.52895289528954\n",
            "Training for 5000 iterations complete, Loss = 6860.723564633563\n",
            "Training for 6000 iterations complete,\n",
            "Training for 7000 iterations complete,\n",
            "Training for 8000 iterations complete,\n",
            "Training for 9000 iterations complete,\n",
            "Accuarcy Test:  90.3990399039904\n",
            "Training for 10000 iterations complete, Loss = 5957.219438549036\n",
            "Training for 11000 iterations complete,\n",
            "Training for 12000 iterations complete,\n",
            "Training for 13000 iterations complete,\n",
            "Training for 14000 iterations complete,\n",
            "Accuarcy Test:  92.81928192819282\n",
            "Training for 15000 iterations complete, Loss = 4115.250250755962\n",
            "Training for 16000 iterations complete,\n",
            "Training for 17000 iterations complete,\n",
            "Training for 18000 iterations complete,\n",
            "Training for 19000 iterations complete,\n",
            "Accuarcy Test:  93.99939993999399\n",
            "Training for 20000 iterations complete, Loss = 3365.6172934253027\n",
            "Training for 21000 iterations complete,\n",
            "Training for 22000 iterations complete,\n",
            "Training for 23000 iterations complete,\n",
            "Training for 24000 iterations complete,\n",
            "Accuarcy Test:  94.82948294829482\n",
            "Training for 25000 iterations complete, Loss = 2889.005723525811\n",
            "Training for 26000 iterations complete,\n",
            "Training for 27000 iterations complete,\n",
            "Training for 28000 iterations complete,\n",
            "Training for 29000 iterations complete,\n",
            "Accuarcy Test:  95.05950595059505\n",
            "Training for 30000 iterations complete, Loss = 2539.971147380239\n",
            "Training for 31000 iterations complete,\n",
            "Training for 32000 iterations complete,\n",
            "Training for 33000 iterations complete,\n",
            "Training for 34000 iterations complete,\n",
            "Accuarcy Test:  95.2895289528953\n",
            "Training for 35000 iterations complete, Loss = 2227.9751028259197\n",
            "Training for 36000 iterations complete,\n",
            "Training for 37000 iterations complete,\n",
            "Training for 38000 iterations complete,\n",
            "Training for 39000 iterations complete,\n",
            "Accuarcy Test:  95.73957395739573\n",
            "Training for 40000 iterations complete, Loss = 2129.2823125103937\n",
            "Training for 41000 iterations complete,\n",
            "Training for 42000 iterations complete,\n",
            "Training for 43000 iterations complete,\n",
            "Training for 44000 iterations complete,\n",
            "Accuarcy Test:  96.14961496149616\n",
            "Training for 45000 iterations complete, Loss = 1762.6383617113\n",
            "Training for 46000 iterations complete,\n",
            "Training for 47000 iterations complete,\n",
            "Training for 48000 iterations complete,\n",
            "Training for 49000 iterations complete,\n",
            "Accuarcy Test:  95.6995699569957\n",
            "Training for 50000 iterations complete, Loss = 1710.0998110566422\n",
            "Training for 51000 iterations complete,\n",
            "Training for 52000 iterations complete,\n",
            "Training for 53000 iterations complete,\n",
            "Training for 54000 iterations complete,\n",
            "Accuarcy Test:  96.53965396539654\n",
            "Training for 55000 iterations complete, Loss = 1417.9811368544138\n",
            "Training for 56000 iterations complete,\n",
            "Training for 57000 iterations complete,\n",
            "Training for 58000 iterations complete,\n",
            "Training for 59000 iterations complete,\n",
            "Accuarcy Test:  96.15961596159616\n",
            "Training for 60000 iterations complete, Loss = 1447.1389184190982\n",
            "Training for 61000 iterations complete,\n",
            "Training for 62000 iterations complete,\n",
            "Training for 63000 iterations complete,\n",
            "Training for 64000 iterations complete,\n",
            "Accuarcy Test:  96.57965796579659\n",
            "Training for 65000 iterations complete, Loss = 1233.7593955879931\n",
            "Training for 66000 iterations complete,\n",
            "Training for 67000 iterations complete,\n",
            "Training for 68000 iterations complete,\n",
            "Training for 69000 iterations complete,\n",
            "Accuarcy Test:  96.23962396239624\n",
            "Training for 70000 iterations complete, Loss = 1256.2141380948783\n",
            "Training for 71000 iterations complete,\n",
            "Training for 72000 iterations complete,\n",
            "Training for 73000 iterations complete,\n",
            "Training for 74000 iterations complete,\n",
            "Accuarcy Test:  96.6996699669967\n",
            "Training for 75000 iterations complete, Loss = 1104.864072507077\n",
            "Training for 76000 iterations complete,\n",
            "Training for 77000 iterations complete,\n",
            "Training for 78000 iterations complete,\n",
            "Training for 79000 iterations complete,\n",
            "Accuarcy Test:  96.18961896189619\n",
            "Training for 80000 iterations complete, Loss = 1247.8776191159618\n",
            "Training for 81000 iterations complete,\n",
            "Training for 82000 iterations complete,\n",
            "Training for 83000 iterations complete,\n",
            "Training for 84000 iterations complete,\n",
            "Accuarcy Test:  96.66966696669667\n",
            "Training for 85000 iterations complete, Loss = 1011.836002250792\n",
            "Training for 86000 iterations complete,\n",
            "Training for 87000 iterations complete,\n",
            "Training for 88000 iterations complete,\n",
            "Training for 89000 iterations complete,\n",
            "Accuarcy Test:  96.62966296629664\n",
            "Training for 90000 iterations complete, Loss = 901.4957414326996\n",
            "Training for 91000 iterations complete,\n",
            "Training for 92000 iterations complete,\n",
            "Training for 93000 iterations complete,\n",
            "Training for 94000 iterations complete,\n",
            "Accuarcy Test:  97.03970397039704\n",
            "Training for 95000 iterations complete, Loss = 793.0058931934205\n",
            "Training for 96000 iterations complete,\n",
            "Training for 97000 iterations complete,\n",
            "Training for 98000 iterations complete,\n",
            "Training for 99000 iterations complete,\n",
            "Accuarcy Test:  96.86968696869687\n",
            "Training for 100000 iterations complete, Loss = 829.3054299935765\n",
            "Training for 101000 iterations complete,\n",
            "Training for 102000 iterations complete,\n",
            "Training for 103000 iterations complete,\n",
            "Training for 104000 iterations complete,\n",
            "Accuarcy Test:  96.68966896689669\n",
            "Training for 105000 iterations complete, Loss = 773.9863406810557\n",
            "Training for 106000 iterations complete,\n",
            "Training for 107000 iterations complete,\n",
            "Training for 108000 iterations complete,\n",
            "Training for 109000 iterations complete,\n",
            "Accuarcy Test:  96.999699969997\n",
            "Training for 110000 iterations complete, Loss = 690.5247397094105\n",
            "Training for 111000 iterations complete,\n",
            "Training for 112000 iterations complete,\n",
            "Training for 113000 iterations complete,\n",
            "Training for 114000 iterations complete,\n",
            "Accuarcy Test:  96.78967896789679\n",
            "Training for 115000 iterations complete, Loss = 869.0765639997119\n",
            "Training for 116000 iterations complete,\n",
            "Training for 117000 iterations complete,\n",
            "Training for 118000 iterations complete,\n",
            "Training for 119000 iterations complete,\n",
            "Accuarcy Test:  96.44964496449644\n",
            "Training for 120000 iterations complete, Loss = 800.0125454655539\n",
            "Training for 121000 iterations complete,\n",
            "Training for 122000 iterations complete,\n",
            "Training for 123000 iterations complete,\n",
            "Training for 124000 iterations complete,\n",
            "Accuarcy Test:  97.20972097209722\n",
            "Training for 125000 iterations complete, Loss = 491.703796570881\n",
            "Training for 126000 iterations complete,\n",
            "Training for 127000 iterations complete,\n",
            "Training for 128000 iterations complete,\n",
            "Training for 129000 iterations complete,\n",
            "Accuarcy Test:  97.00970097009701\n",
            "Training for 130000 iterations complete, Loss = 487.08456941779366\n",
            "Training for 131000 iterations complete,\n",
            "Training for 132000 iterations complete,\n",
            "Training for 133000 iterations complete,\n",
            "Training for 134000 iterations complete,\n",
            "Accuarcy Test:  97.08970897089709\n",
            "Training for 135000 iterations complete, Loss = 517.2508941790381\n",
            "Training for 136000 iterations complete,\n",
            "Training for 137000 iterations complete,\n",
            "Training for 138000 iterations complete,\n",
            "Training for 139000 iterations complete,\n",
            "Accuarcy Test:  97.06970697069707\n",
            "Training for 140000 iterations complete, Loss = 464.5173986740603\n",
            "Training for 141000 iterations complete,\n",
            "Training for 142000 iterations complete,\n",
            "Training for 143000 iterations complete,\n",
            "Training for 144000 iterations complete,\n",
            "Accuarcy Test:  97.30973097309732\n",
            "Training for 145000 iterations complete, Loss = 361.32244114799346\n",
            "Training for 146000 iterations complete,\n",
            "Training for 147000 iterations complete,\n",
            "Training for 148000 iterations complete,\n",
            "Training for 149000 iterations complete,\n",
            "Accuarcy Test:  97.25972597259727\n",
            "Training for 150000 iterations complete, Loss = 335.43827285629584\n",
            "Training for 151000 iterations complete,\n",
            "Training for 152000 iterations complete,\n",
            "Training for 153000 iterations complete,\n",
            "Training for 154000 iterations complete,\n",
            "Accuarcy Test:  97.20972097209722\n",
            "Training for 155000 iterations complete, Loss = 363.8683971740815\n",
            "Training for 156000 iterations complete,\n",
            "Training for 157000 iterations complete,\n",
            "Training for 158000 iterations complete,\n",
            "Training for 159000 iterations complete,\n",
            "Accuarcy Test:  97.13971397139714\n",
            "Training for 160000 iterations complete, Loss = 423.70277994991443\n",
            "Training for 161000 iterations complete,\n",
            "Training for 162000 iterations complete,\n",
            "Training for 163000 iterations complete,\n",
            "Training for 164000 iterations complete,\n",
            "Accuarcy Test:  97.01970197019702\n",
            "Training for 165000 iterations complete, Loss = 381.1351905125098\n",
            "Training for 166000 iterations complete,\n",
            "Training for 167000 iterations complete,\n",
            "Training for 168000 iterations complete,\n",
            "Training for 169000 iterations complete,\n",
            "Accuarcy Test:  97.27972797279728\n",
            "Training for 170000 iterations complete, Loss = 240.89861510316126\n",
            "Training for 171000 iterations complete,\n",
            "Training for 172000 iterations complete,\n",
            "Training for 173000 iterations complete,\n",
            "Training for 174000 iterations complete,\n",
            "Accuarcy Test:  97.2997299729973\n",
            "Training for 175000 iterations complete, Loss = 246.33262236782258\n",
            "Training for 176000 iterations complete,\n",
            "Training for 177000 iterations complete,\n",
            "Training for 178000 iterations complete,\n",
            "Training for 179000 iterations complete,\n",
            "Accuarcy Test:  97.1897189718972\n",
            "Training for 180000 iterations complete, Loss = 258.81964721631\n",
            "Training for 181000 iterations complete,\n",
            "Training for 182000 iterations complete,\n",
            "Training for 183000 iterations complete,\n",
            "Training for 184000 iterations complete,\n",
            "Accuarcy Test:  97.45974597459745\n",
            "Training for 185000 iterations complete, Loss = 194.36359372551573\n",
            "Training for 186000 iterations complete,\n",
            "Training for 187000 iterations complete,\n",
            "Training for 188000 iterations complete,\n",
            "Training for 189000 iterations complete,\n",
            "Accuarcy Test:  97.32973297329733\n",
            "Training for 190000 iterations complete, Loss = 187.85341828660987\n",
            "Training for 191000 iterations complete,\n",
            "Training for 192000 iterations complete,\n",
            "Training for 193000 iterations complete,\n",
            "Training for 194000 iterations complete,\n",
            "Accuarcy Test:  97.53975397539755\n",
            "Training for 195000 iterations complete, Loss = 180.4780986231328\n",
            "Training for 196000 iterations complete,\n",
            "Training for 197000 iterations complete,\n",
            "Training for 198000 iterations complete,\n",
            "Training for 199000 iterations complete,\n",
            "Accuarcy Test:  97.22972297229722\n",
            "Training for 200000 iterations complete, Loss = 197.43987066712458\n",
            "Training finished!\n",
            "Accuarcy Test:  97.22972297229722\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIZgseZlgfFh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}